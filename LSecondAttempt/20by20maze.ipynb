{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Imports\n",
    "\n",
    "- numpy will be handy for mathematical functions.\n",
    "- matplotlib will be used for the visualisation of the maze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Maze\n",
    "\n",
    "- Using matplotplib we use arrays filled with zeros (open space) and ones (walls) to build the maze structure. \n",
    "- In here, we also define the positions of the starting area, the end goal, and the sub goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the maze\n",
    "class Maze:\n",
    "    def __init__(self, maze, start_position, goal_position, sub_goal_position):\n",
    "        self.maze = maze\n",
    "        \n",
    "        self.maze_width = maze_struc.shape[1]           # rows of maze, also knows as x-azis. \n",
    "        self.maze_height = maze_struc.shape[0]          # columns of maze, also known as y-axis.\n",
    "\n",
    "        self.start_position = start_position            # start position - S.\n",
    "        self.goal_position = goal_position              # end goal position - E.\n",
    "        self.sub_goal_position = sub_goal_position      # sub goal position - G.\n",
    "\n",
    "    def show_maze(self):\n",
    "        plt.figure(figsize=(5,5))\n",
    "\n",
    "        plt.imshow(self.maze, cmap='Pastel1')\n",
    "\n",
    "        # Placements for the start, end, and sub goal positions.\n",
    "        plt.text(self.start_position[0], self.start_position[1], 'S', ha='center', va='center', color='green', fontsize=10)\n",
    "        plt.text(self.goal_position[0], self.goal_position[1], 'E', ha='center', va='center', color='red', fontsize=10)\n",
    "        plt.text(self.sub_goal_position[0], self.sub_goal_position[1], 'G', ha='center', va='center', color='blue', fontsize=10)\n",
    "\n",
    "        # Add grid lines between every wall/space.\n",
    "        plt.grid(color='black', linestyle='-', linewidth=0.5)\n",
    "        plt.xticks(np.arange(0.5, self.maze.shape[1], 1))\n",
    "        plt.yticks(np.arange(0.5, self.maze.shape[0], 1))\n",
    "        plt.gca().set_xticks(np.arange(-0.5, self.maze.shape[1], 1), minor=True)\n",
    "        plt.gca().set_yticks(np.arange(-0.5, self.maze.shape[0], 1), minor=True)\n",
    "        plt.gca().grid(which='minor', color='grey', linestyle='-', linewidth=0.5)\n",
    "        plt.gca().tick_params(which='both', length=0)\n",
    "\n",
    "        plt.xlim(-0.45, self.maze.shape[1] - 0.5)\n",
    "        plt.ylim(self.maze.shape[0] - 0.5, -0.35)\n",
    "\n",
    "        # Hide the digits and labels from the plot visualistion. \n",
    "        plt.xticks([]), plt.yticks([])\n",
    "\n",
    "        # Ensures the plot will visualise when running the code.\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# The layout of the 10x10 maze:\n",
    "# 1 = wall.\n",
    "# 0 = open area.\n",
    "# (If wanted, we can later make another file where we can generate larger mazes like 100x100).\n",
    "maze_struc = np.array([\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "[1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "[1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1],\n",
    "[1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "[1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1],\n",
    "[1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
    "[1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1],\n",
    "[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1],\n",
    "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1],\n",
    "[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
    "[1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1],\n",
    "[1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "[1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1],\n",
    "[1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1],\n",
    "[1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1],\n",
    "[1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "[1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1],\n",
    "[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1],\n",
    "[1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "# Places the start, end, and sub goal at the correct coordinates (rows, columns).\n",
    "maze = Maze(maze_struc, (1, 1), (18, 18), (1,7)) #start, end, sub goal\n",
    "\n",
    "# Actually visualises the plot with matplotlib.\n",
    "maze.show_maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Agent\n",
    "\n",
    "This is where the logic of the agent come in:\n",
    "- The agent can move in four directions. These are up, down, left, and right. \n",
    "- The q learning formula and logic will be applied to it. The parameters the agent will go off are:\n",
    "    - the learning rate → decides how much new information overrides over information.\n",
    "    - the discount factor → decides if the agent will prefer better rewards later on, or smaller awards right away\n",
    "    - exploration rate → exploration vs. exploitation. This code works with a decay, over time it will proper exploitation over exploration in the maze.\n",
    "\n",
    "- The agent is based on this formula of Q-Learning:\n",
    "\n",
    "$$ Q(s, a) ← Q(s, a) + α [r + γ max(a') Q(s', a') - Q(s, a)] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions the agent can take.\n",
    "moves = [\n",
    "   (-1, 0),         # Moving one step up.\n",
    "   (1, 0),          # Moving one step down.\n",
    "   (0, -1),         # Moving one step left.\n",
    "   (0, 1)           # Moving one step right.\n",
    "]\n",
    "\n",
    "# Initialise the Q-Learning agent \n",
    "class QLearningAgent:\n",
    "    def __init__(self, maze, learning_rate=0.1, discount_factor=0.9, exploration_start=1.0, exploration_end=0.5, num_episodes=100):\n",
    "        \n",
    "        # The table gets updated as new information is stored. 4 stands for the actions the agent can take. \n",
    "        self.q_table = np.zeros((maze.maze_height, maze.maze_width, 4)) \n",
    "         \n",
    "        self.learning_rate = learning_rate          \n",
    "        self.discount_factor = discount_factor      \n",
    "        self.exploration_start = exploration_start  \n",
    "        self.exploration_end = exploration_end\n",
    "        self.num_episodes = num_episodes\n",
    "\n",
    "    # Calculates the rate of exploration to exploitation over time -> start with a lot of exploration and eventually prefer exploitation.\n",
    "    def get_exploration_rate(self, current_episode):\n",
    "        exploration_rate = self.exploration_start * (self.exploration_end / self.exploration_start) ** (current_episode / self.num_episodes)\n",
    "        return exploration_rate\n",
    "    \n",
    "    # Chooses what movement action to make. \n",
    "    def get_action(self, state, current_episode):\n",
    "        exploration_rate = self.get_exploration_rate(current_episode)\n",
    "\n",
    "        # Select an action for the given state either randomly (exploration) or using the Q-table (exploitation).\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.randint(4) \n",
    "        else:\n",
    "            #Chooses the action with the highest Q-value for the given state.\n",
    "            return np.argmax(self.q_table[state]) \n",
    "        \n",
    "    # Updates the Q-values in the Q-table based on its actions and states.\n",
    "    def update_q_table(self, state, action, next_state, reward):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "\n",
    "        current_q_value = self.q_table[state][action]\n",
    "\n",
    "        # Formula to update the Q-value based on the theory of the Q-Learning algorithm.\n",
    "        new_q_value = current_q_value + self.learning_rate * (reward + self.discount_factor * self.q_table[next_state][best_next_action] - current_q_value)\n",
    "\n",
    "        # Apply new Q-value for current action and state. \n",
    "        self.q_table[state][action] = new_q_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Agent in Maze\n",
    "\n",
    "This cell will show the behaviour of the agent in a single episode and before it is trained with the Q-Learning algorithm.\n",
    "\n",
    "- It DOES define the logic on the agent inside of the maze. In other words, giving the agent rewards and penalities for hitting walls and reaching the goal, respectively.\n",
    "- So, it keeps track of the total reward.\n",
    "- It also tracks the total steps the agent took.\n",
    "- It also updates the Q-values as necessary.\n",
    "\n",
    "Below, we first initialise the rewards or penalties the agent will receive based on its behaviour. Change as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards.\n",
    "goal_reward = 700\n",
    "sub_goal_reward = 300\n",
    "\n",
    "# Penalities.\n",
    "wall_penalty = -5\n",
    "step_penalty = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning_logic(agent, maze, current_episode, train=True):\n",
    "\n",
    "    current_state = maze.start_position     # Agent starts at start position.\n",
    "    path = [current_state]                  # Tracks the agent's current position.\n",
    "\n",
    "    goal_reached = False                    # Tracks if the agent reached the goal.\n",
    "    sub_reached = False                     # Tracks if the agent reached the sub goal.\n",
    "\n",
    "    episode_reward = 0                      # Tracks the agent's total reward at the end of the episode.\n",
    "    episode_step = 0                        # Tracks the agent's total steps at the end of the episode.\n",
    "    \n",
    "    while not goal_reached:\n",
    "\n",
    "        # Decide the agent's next action based on the Q-Table.\n",
    "        action = agent.get_action(current_state, current_episode)\n",
    "        next_state = (current_state[0] + moves[action][0], current_state[1] + moves[action][1])\n",
    "\n",
    "        # Give a penalty if a wall is hit.\n",
    "        if (next_state[0] < 0 or next_state[0] >= maze.maze_height or \n",
    "            next_state[1] < 0 or next_state[1] >= maze.maze_width or \n",
    "            maze.maze[next_state[1]][next_state[0]] == 1):\n",
    "            reward = wall_penalty\n",
    "            next_state = current_state\n",
    "\n",
    "        # Give a single-time reward if the sub-goal is reached.\n",
    "        elif next_state == maze.sub_goal_position and not sub_reached:\n",
    "            path.append(current_state)\n",
    "            reward = sub_goal_reward\n",
    "            sub_reached = True\n",
    "\n",
    "        # Mark that the agent has reached the end and give a reward.\n",
    "        elif next_state == maze.goal_position:\n",
    "            path.append(current_state)\n",
    "            reward = goal_reward\n",
    "            goal_reached = True\n",
    "\n",
    "        # Give a penalty every time the agent takes a step without reaching the final goal position.\n",
    "        else:\n",
    "            path.append(current_state)\n",
    "            reward = step_penalty\n",
    "\n",
    "        # Track the total steps and reward.\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "\n",
    "        # Update Q-table if training is set to True.\n",
    "        if train:\n",
    "            agent.update_q_table(current_state, action, next_state, reward)\n",
    "\n",
    "        # Update the agent's current position.\n",
    "        current_state = next_state\n",
    "\n",
    "    # Return total reward, steps, path, and whether the sub-goal was reached.\n",
    "    return episode_reward, episode_step, path, sub_reached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Agent Test\n",
    "\n",
    "- This version shows how the agent behaves in a single episode without any training whatsoever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to visualise the maze.\n",
    "def visualize_maze(maze, path, sub_goal_reached):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(maze.maze, cmap='Pastel1')\n",
    "    \n",
    "    plt.text(maze.start_position[0], maze.start_position[1], 'S', ha='center', va='center', color='green', fontsize=10)\n",
    "    plt.text(maze.goal_position[0], maze.goal_position[1], 'E', ha='center', va='center', color='red', fontsize=10)\n",
    "    plt.text(maze.sub_goal_position[0], maze.sub_goal_position[1], 'G', ha='center', va='center', color='blue', fontsize=10)\n",
    "\n",
    "    plt.grid(color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.xticks(np.arange(0.5, maze.maze.shape[1], 1))\n",
    "    plt.yticks(np.arange(0.5, maze.maze.shape[0], 1))\n",
    "    plt.gca().set_xticks(np.arange(-0.5, maze.maze.shape[1], 1), minor=True)\n",
    "    plt.gca().set_yticks(np.arange(-0.5, maze.maze.shape[0], 1), minor=True)\n",
    "    plt.gca().grid(which='minor', color='grey', linestyle='-', linewidth=0.5)\n",
    "    plt.gca().tick_params(which='both', length=0)\n",
    "\n",
    "    plt.xlim(-0.45, maze.maze.shape[1] - 0.5)\n",
    "    plt.ylim(maze.maze.shape[0] - 0.5, -0.35)\n",
    "\n",
    "    # Additionally show the path the agent took.\n",
    "    for position in path:\n",
    "        plt.text(position[0], position[1], \"●\", va='center', color='white', fontsize=5)\n",
    "\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show(block=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows a single episode of the agent (untrained).\n",
    "def test_agent(agent, maze, num_episodes=1):\n",
    "    episode_reward, episode_step, path, sub_reached = qlearning_logic(agent, maze, num_episodes, train=False)\n",
    "\n",
    "    # Shows the total steps and total reward.\n",
    "    print(\"Total steps:\", episode_step)\n",
    "    print(\"Total reward:\", episode_reward)\n",
    "\n",
    "    # Visualize the maze with the agent's path.\n",
    "    visualize_maze(maze, path, sub_reached)\n",
    "\n",
    "    return episode_step, episode_reward\n",
    "\n",
    "agent = QLearningAgent(maze)\n",
    "test_agent(agent, maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent in the Maze\n",
    "\n",
    "- We are now going to train the agent. By saving it's previous attempts and learning from the information it learned, it should improve its behaviour over time over the course of its assigned number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, maze, num_episodes=100):\n",
    "    \n",
    "    # Store the data for the rewards and the steps (for plotting purposes).\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "\n",
    "    # Loop over the number of episodes.\n",
    "    for episode in range(num_episodes):\n",
    "        episode_reward, episode_step, path, sub_reached = qlearning_logic(agent, maze, episode, train=True)\n",
    "\n",
    "        # Add the rewards and steps to their respective lists.\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(episode_step)\n",
    "\n",
    "    # Training data will be plotted here.\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Reward per Episode')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(episode_steps)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps Taken')\n",
    "    plt.ylim(0, 500)\n",
    "    plt.title('Steps per Episode')\n",
    "\n",
    "    # Print the final reward and steps after training finished.\n",
    "    final_reward = episode_rewards[-1]\n",
    "    final_steps = episode_steps[-1]\n",
    "\n",
    "    print(f\"The final reward after training is: {final_reward}\")\n",
    "    print(f\"The final steps after training are: {final_steps}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the agent\n",
    "train_agent(agent, maze, num_episodes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Trained Agent\n",
    "\n",
    "- As you will probably see, if everything went well, the agent should now traverse through the maze the right way (fewest steps and highest rewards) after running the code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the agent after training\n",
    "test_agent(agent, maze, num_episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
